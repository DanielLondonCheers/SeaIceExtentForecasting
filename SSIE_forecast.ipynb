{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code to accompany paper: \"Regional September Sea Ice Forecasting with Complex Networks\"\n",
    "### Author: William Gregory\n",
    "### Code Last updated: 22/04/2019\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "import glob\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import math\n",
    "import operator\n",
    "from scipy import stats\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import scipy\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.optimize import minimize\n",
    "from Complex_Networks import CN\n",
    "\n",
    "#G L O B A L   V A R I A B L E S\n",
    "\n",
    "hdr = 300\n",
    "dimX = 448 #Polar stereo grid dimensions (25km)\n",
    "dimY = 304\n",
    "\n",
    "dimXR = 57 #downsampled Polar stereo (100km)\n",
    "dimYR = 57\n",
    "\n",
    "datapath='./Data/'\n",
    "corrpath='./Correlations/'\n",
    "\n",
    "latf = datapath+'psn25lats_v3.dat'\n",
    "lat = np.fromfile(latf,dtype='<i4')\n",
    "lat = lat.reshape(448,304)\n",
    "lat = lat/100000\n",
    "\n",
    "lonf = datapath+'psn25lons_v3.dat'\n",
    "lon = np.fromfile(lonf,dtype='<i4')\n",
    "lon = lon.reshape(448,304)\n",
    "lon = lon/100000\n",
    "\n",
    "regions = ['total', 'Beaufort_Sea', 'Chukchi_Sea', 'East_Siberian_Sea', 'Laptev_Sea', 'Kara_Sea', 'Barents_Sea', 'Greenland_Sea', 'Baffin_Bay', 'Canadian_Arch']\n",
    "SIEs = {}\n",
    "SIEs_dt = {}\n",
    "SIEs_trend = {}\n",
    "for k in range(len(regions)):\n",
    "    tag = regions[k]\n",
    "    if tag == 'total':\n",
    "        file = np.genfromtxt(datapath+'september_SIE.txt')\n",
    "        SIEs.setdefault(tag, []).append(file)\n",
    "    else:\n",
    "        file = np.genfromtxt(datapath+'september_SIE_'+str(tag)+'.txt')\n",
    "        SIEs.setdefault(tag, []).append(file)\n",
    "for k in range(len(regions)):\n",
    "    tag = regions[k]\n",
    "    trend = np.zeros((2018-1984+1,2))\n",
    "    dt = np.zeros((2018-1984+1,2018-1979+1))\n",
    "    for yend in range(1984,2018+1):\n",
    "        nmax = yend - 1979\n",
    "        trendT, interceptT, r_valsT, probT, stderrT = stats.linregress(np.arange(nmax+1),SIEs[tag][0][range(nmax+1)])\n",
    "        lineT = (trendT*np.arange(nmax+1)) + interceptT\n",
    "        trend[yend-1984,0] = trendT\n",
    "        trend[yend-1984,1] = interceptT\n",
    "        dt[yend-1984,range(nmax+1)] = SIEs[tag][0][range(nmax+1)]-lineT\n",
    "    SIEs_trend.setdefault(tag, []).append(trend)\n",
    "    SIEs_dt.setdefault(tag, []).append(dt)\n",
    "\n",
    "#D E F I N E   C L A S S E S\n",
    "\n",
    "class SIC_daily:\n",
    "    def __init__(self, data, monthly):\n",
    "        self.data = data\n",
    "        self.monthly = monthly\n",
    "\n",
    "    def read_daily(self, month, days, ymax):\n",
    "            j = -1\n",
    "            for y in range(2017,ymax+1):\n",
    "                j = j + 1\n",
    "                k = -1\n",
    "                for d in range(1,days+1):\n",
    "                    k = k + 1\n",
    "                    icefile = open(glob.glob(datapath+'nt_'+str(y)+str(month)+str('%02d'%d)+'*.bin')[0], 'rb')\n",
    "                    contents = icefile.read()\n",
    "                    icefile.close()\n",
    "                    s=\"%dB\" % (int(dimX*dimY),)\n",
    "                    z=struct.unpack_from(s, contents, offset = hdr)\n",
    "                    self.data[:,:,j,k] = np.array(z).reshape((dimX,dimY))\n",
    "            self.data = self.data/250\n",
    "            self.data[self.data>1]=np.nan       \n",
    "\n",
    "    def create_monthly(self, ymax):\n",
    "        for i,j in itertools.product(range(dimX),range(dimY)):\n",
    "            for y in range(ymax):\n",
    "                self.monthly[i,j,y] = np.nanmean(self.data[i,j,y,:])\n",
    "\n",
    "class SIC_monthly:\n",
    "    def __init__(self, data, regrid, trend, trend_regrid, dt, dt_regrid, nodes, anomalies):\n",
    "        self.data = data\n",
    "        self.regrid = regrid\n",
    "        self.trend = trend\n",
    "        self.trend_regrid = trend_regrid\n",
    "        self.dt = dt\n",
    "        self.dt_regrid = dt_regrid\n",
    "        self.nodes = nodes\n",
    "        self.anomalies = anomalies\n",
    "        \n",
    "    def read_monthly(self, month):\n",
    "        k = 0\n",
    "        for y in range(1979,2016+1):\n",
    "            icefile = open(glob.glob(datapath+'nt_'+str(y)+str(month)+'*.bin')[0], 'rb')\n",
    "            contents = icefile.read()\n",
    "            icefile.close()\n",
    "            s=\"%dB\" % (int(dimX*dimY),)\n",
    "            z=struct.unpack_from(s, contents, offset = hdr)\n",
    "            self.data[:,:,k] = np.array(z).reshape((dimX,dimY))\n",
    "            k = k + 1\n",
    "        self.data = self.data/250\n",
    "        self.data[self.data>1]=np.nan\n",
    "        \n",
    "    def regrid(self, month, lat_ori, lon_ori):\n",
    "        m = Basemap(projection='npstere',boundinglat=65,lon_0=0, resolution='l')\n",
    "        x,y = m(lon_ori,lat_ori)\n",
    "        dx_res = 100000 #100 km square\n",
    "        new_x = int((m.xmax-m.xmin)/dx_res)+1 ; new_y = int((m.ymax-m.ymin)/dx_res)+1\n",
    "        lonsG, latsG = m.makegrid(new_x, new_y)\n",
    "        xt,yt=m(lonsG,latsG)\n",
    "        \n",
    "        for t in range(np.shape(self.data)[2]):\n",
    "            ice_copy = np.copy(self.data[:,:,t])\n",
    "            #SMMR Pole Hole Mask: 84.5 November 1978 - June 1987\n",
    "            #SSM/I Pole Hole Mask: 87.2 July 1987 - December 2007\n",
    "            #SSMIS Pole Hole Mask: 89.18 January 2008 - present\n",
    "            if t < 1987-1979:\n",
    "                pmask=84.5\n",
    "            elif (t == 1987-1979) & (month <= 6):\n",
    "                pmask=84.5\n",
    "            elif (t == 1987-1979) & (month > 6):\n",
    "                pmask=84.5#87.2\n",
    "            elif (t > 1987-1979) & (t < 2008-1979):\n",
    "                pmask=87.2\n",
    "            else:\n",
    "                pmask=89.2\n",
    "            hole = np.nanmean(ice_copy[(lat_ori > pmask-0.5) & (lat_ori < pmask)]) #calculate the mean 0.5 degrees around polar hole\n",
    "            self.data[:,:,t] = np.ma.where((lat_ori >= pmask-0.5), hole, self.data[:,:,t]) #Fill polar hole with mean\n",
    "            self.regrid[:,:,t] = griddata((x.ravel(), y.ravel()),self.data[:,:,t].ravel(), (xt, yt), method='linear') #downsample to 100km\n",
    "        \n",
    "        return lonsG, latsG\n",
    "        \n",
    "    def detrend(self, ymax, data):\n",
    "        self.dt = {}\n",
    "        self.dt_regrid = {}\n",
    "        self.trend = {}\n",
    "        self.trend_regrid = {}\n",
    "        X = data.shape[0] ; Y = data.shape[1]\n",
    "        for yend in range(1984,ymax+1):\n",
    "            nmax = yend - 1979\n",
    "            detrended = np.zeros((dimX,dimY,nmax+1)) ; detrended_regrid = np.zeros((dimXR,dimYR,nmax+1))\n",
    "            detrended[detrended==0] = np.nan ; detrended_regrid[detrended_regrid==0] = np.nan\n",
    "            trend = np.zeros((dimX,dimY,nmax+1,2)) ; trend_regrid = np.zeros((dimXR,dimYR,nmax+1,2))\n",
    "            for i,j in itertools.product(range(X),range(Y)):\n",
    "                if all(~np.isnan(data[i,j,range(nmax+1)])):\n",
    "                    trendT, interceptT, r_valsT, probT, stderrT = stats.linregress(np.arange(nmax+1),data[i,j,range(nmax+1)])\n",
    "                    lineT = (trendT*np.arange(nmax+1)) + interceptT\n",
    "                    if X == 57:\n",
    "                        trend_regrid[i,j,yend-1984,0] = trendT\n",
    "                        trend_regrid[i,j,yend-1984,1] = interceptT\n",
    "                        detrended_regrid[i,j,range(nmax+1)]=data[i,j,range(nmax+1)]-lineT\n",
    "                    else:\n",
    "                        trend[i,j,yend-1984,0] = trendT\n",
    "                        trend[i,j,yend-1984,1] = interceptT\n",
    "                        detrended[i,j,range(nmax+1)]=data[i,j,range(nmax+1)]-lineT\n",
    "\n",
    "            self.dt.setdefault(yend, []).append(detrended)\n",
    "            self.dt_regrid.setdefault(yend, []).append(detrended_regrid)\n",
    "            self.trend.setdefault(yend, []).append(trend)\n",
    "            self.trend_regrid.setdefault(yend, []).append(trend_regrid)\n",
    "                \n",
    "    def gen_networks(self, month, lats):\n",
    "        self.nodes = {}\n",
    "        self.anomalies = {}\n",
    "        for yend in range(1985,2018+1):\n",
    "            print('Creating network: 1979 - ',yend)\n",
    "            nmax = yend - 1979\n",
    "            network = CN.Network(dimX=dimXR,dimY=dimYR)\n",
    "            CN.Network.cell_level(network, self.dt_regrid[yend][0][:,:,range(nmax+1)], str(month), \"_100sqkm_79-\"+str(yend), corrpath)\n",
    "            CN.Network.tau(network, self.dt_regrid[yend][0][:,:,range(nmax+1)], str(month), 0.01, \"_100sqkm_79-\"+str(yend), corrpath)\n",
    "            CN.Network.area_level(network, str(month))\n",
    "            CN.Network.intra_links(network, self.dt_regrid[yend][0][:,:,range(nmax+1)], str(month), lats)\n",
    "            self.nodes.setdefault(yend, []).append(network.V)\n",
    "            self.anomalies.setdefault(yend, []).append(network.anomaly)\n",
    "        \n",
    "class GPR:\n",
    "    def __init__(self, forecast, retrend, error, error_retrend_minus, error_retrend_plus):\n",
    "        self.fmean = forecast\n",
    "        self.fmean_rt = retrend   \n",
    "        self.fvar = error\n",
    "        self.fvar_rt_minus = error_retrend_minus\n",
    "        self.fvar_rt_plus = error_retrend_plus\n",
    "            \n",
    "    def forecast(self, target, trends, month, anomalies, iterations):\n",
    "        print('Running Forecast')\n",
    "        for m in range(len(regions)):\n",
    "            print('Forecasting: ',str(regions[m]),' SIE')\n",
    "            print(datetime.datetime.now())\n",
    "            for yend in range(1985,2018+1):\n",
    "                nmax = yend - 1979\n",
    "                X = np.zeros((nmax,len(anomalies[yend][0]))) #Predictors (n x N)\n",
    "                Z = np.zeros((len(anomalies[yend][0]),1)) #Test case for forecast (N x 1)\n",
    "                M = np.zeros((len(anomalies[yend][0]),len(anomalies[yend][0]))) #Adj matrix for Prior Covariance (N x N)\n",
    "                #print('Forecast year: ',yend)\n",
    "                k = -1\n",
    "                for area in anomalies[yend][0]: #For each network node\n",
    "                    k = k + 1\n",
    "                    X[:,k] = anomalies[yend][0][area][0][range(nmax)] \n",
    "                    Z[k,0] = anomalies[yend][0][area][0][nmax]\n",
    "                    l = -1\n",
    "                    for area2 in anomalies[yend][0]: #Repeat loop to generate network links\n",
    "                        l = l + 1\n",
    "                        if area != area2:\n",
    "                            M[k,l] = np.cov(anomalies[yend][0][area][0][range(nmax)],anomalies[yend][0][area2][0][range(nmax)],bias=True)[0][1]\n",
    "                Xt = X.T\n",
    "                m_prior = np.zeros((X.shape[1],1)) #Zero mean prior (N x 1)\n",
    "                M[M<0] = 0 #only take positive correlations between network nodes\n",
    "                for i in range(len(anomalies[yend][0])):\n",
    "                    ii = -1*(np.nansum(M[i,:]))\n",
    "                    for j in range(len(anomalies[yend][0])):\n",
    "                        if np.isnan(M[i,j]):\n",
    "                            M[i,j] = 0\n",
    "                        elif i == j:\n",
    "                            M[i,j] = ii #set diagonal elements to be the -sum of all link weights\n",
    "\n",
    "                def gen_matrices(C, V):\n",
    "                    mat1 = np.matmul(X,C)\n",
    "                    mat2 = np.matmul(mat1,Xt) + V\n",
    "                    mat2i = np.linalg.inv(mat2)\n",
    "                    mat3 = y - np.matmul(X,m_prior)\n",
    "                    mat4 = np.matmul(mat2i,mat3)\n",
    "                    yKy = np.matmul(mat3.T,mat4)\n",
    "                    evidence = -1*(-0.5*(np.log(2*math.pi)) - yKy/(2*nmax) - np.log(np.linalg.det(mat2))/(2*nmax)) #eq 6 Sollich\n",
    "                    return evidence, yKy, mat2i, mat4\n",
    "\n",
    "                def marginal_likelihood(hyperparameters):\n",
    "                    gradient = np.zeros(2)\n",
    "                    Y = y - np.matmul(X,m_prior)\n",
    "                    sigma = np.exp(hyperparameters[0]) ; l = np.exp(hyperparameters[1])\n",
    "                    C = scipy.linalg.expm(M*l) ; V = np.eye(nmax) * sigma\n",
    "                    evidence, yKy, M1, M2 = gen_matrices(C, V)\n",
    "                    a = yKy/nmax\n",
    "                    Cgrad_sig = a * scipy.linalg.expm(M*l) ; Vgrad_sig = np.eye(nmax) * a\n",
    "                    #Cgrad_sig = a * scipy.linalg.expm(M*l) ; Vgrad_sig = np.eye(nmax) * (2*a*np.sqrt(sigma))\n",
    "                    Cgrad_l = a * np.multiply(scipy.linalg.expm(M*l),M) ; Vgrad_l = np.eye(nmax) * (a*sigma)\n",
    "                    dkdsig = np.matmul(np.matmul(X,Cgrad_sig),Xt) + Vgrad_sig\n",
    "                    dkdl = np.matmul(np.matmul(X,Cgrad_l),Xt) + Vgrad_l\n",
    "                    C = a * scipy.linalg.expm(M*l) ; V = np.eye(nmax) * (a*sigma)\n",
    "                    evidence, yKy, M1, M2 = gen_matrices(C, V)\n",
    "                    gradient[0] = -1*(0.5*np.matmul(np.matmul(np.matmul(Y.T,M1),dkdsig),M2) - 0.5*np.trace(np.matmul(M1,dkdsig)))\n",
    "                    gradient[1] = -1*(0.5*np.matmul(np.matmul(np.matmul(Y.T,M1),dkdl),M2) - 0.5*np.trace(np.matmul(M1,dkdl)))\n",
    "                    return evidence, gradient\n",
    "\n",
    "                y = np.reshape(target[regions[m]][0][yend-1984-1,range(nmax)],(nmax,1)) \n",
    "                #optimise hyperparameters sigma and l\n",
    "                theta_sig = np.zeros(iterations) ; theta_l = np.zeros(iterations) ; evidence = np.zeros(iterations) ; evidence[evidence==0] = np.nan\n",
    "                passed = False\n",
    "                while passed == False:\n",
    "                    for it in range(iterations):\n",
    "                        theta_sig[it] = np.random.uniform(0.001,10)\n",
    "                        theta_l[it] = np.random.uniform(0.001,100)\n",
    "                        try:\n",
    "                            result = scipy.optimize.minimize(marginal_likelihood, [np.log(theta_sig[it]), np.log(theta_l[it])], method='TNC', jac=True, options={'disp':False})\n",
    "                            if result.success == True: #did the result converge?\n",
    "                                evidence[it] = result.fun\n",
    "                            else:\n",
    "                                evidence[it] = np.nan\n",
    "                        except ValueError:\n",
    "                            evidence[it] = np.nan\n",
    "                        except OverflowError:\n",
    "                            evidence[it] = np.nan\n",
    "                        except np.linalg.LinAlgError:\n",
    "                            evidence[it] = np.nan\n",
    "                    if ~np.isnan(evidence).all():\n",
    "                        id0 = np.where(evidence==np.nanmin(evidence))\n",
    "                        id0 = id0[0][0]\n",
    "                        try:\n",
    "                            result = scipy.optimize.minimize(marginal_likelihood, [np.log(theta_sig[id0]), np.log(theta_l[id0])], method='TNC', jac=True, options={'disp':False})\n",
    "                            passed = True\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                        except OverflowError:\n",
    "                            pass\n",
    "                        except np.linalg.LinAlgError:\n",
    "                            pass\n",
    "                sigma = np.exp(result.x[0]) ; l = np.exp(result.x[1])\n",
    "                C = scipy.linalg.expm(M*l) ; V = np.eye(nmax) * sigma\n",
    "                evidence, yKy, M1, M2 = gen_matrices(C, V)\n",
    "                a = yKy/nmax\n",
    "                C = a * scipy.linalg.expm(M*l) ; V = np.eye(nmax) * (a*sigma)\n",
    "                evidence, yKy, M1, M2 = gen_matrices(C, V)\n",
    "                mat1 = np.matmul(C,Xt)\n",
    "                mat2 = np.matmul(mat1,M2)\n",
    "                mat3 = np.matmul(X,C)\n",
    "\n",
    "                m_posterior = m_prior + mat2 #eq 3.37 tarantola\n",
    "                C_posterior = C - np.matmul(np.matmul(mat1,M1),mat3) #eq 3.38 tarantola\n",
    "\n",
    "                mat_a = np.matmul(Z.T,C)\n",
    "                mat_b = np.matmul(mat_a,Z)\n",
    "                mat_c = np.matmul(Z.T,mat1)\n",
    "                mat_d = np.matmul(mat3,Z)\n",
    "                mat_e = np.matmul(mat_c,M1)\n",
    "\n",
    "                self.fmean[m,yend-1985] = np.matmul(Z.T,m_posterior)\n",
    "                self.fvar[m,yend-1985] = mat_b - np.matmul(mat_e,mat_d)\n",
    "                lineT = (np.arange(nmax+1)*trends[regions[m]][0][yend-1984-1,0]) + trends[regions[m]][0][yend-1984-1,1]\n",
    "                self.fmean_rt[m,yend-1985] = self.fmean[m,yend-1985] + lineT[-1]\n",
    "                self.fvar_rt_minus[m,yend-1985] = (self.fmean[m,yend-1985] - np.sqrt(self.fvar[m,yend-1985])) + lineT[-1]\n",
    "                self.fvar_rt_plus[m,yend-1985] = (self.fmean[m,yend-1985] + np.sqrt(self.fvar[m,yend-1985])) + lineT[-1]\n",
    "\n",
    "        print('Done')\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "\n",
    "print('Reading...')\n",
    "daily_SIC_june = SIC_daily(np.zeros((dimX,dimY,2,30)),np.zeros((dimX,dimY,2)))\n",
    "daily_SIC_july = SIC_daily(np.zeros((dimX,dimY,2,31)),np.zeros((dimX,dimY,2)))\n",
    "daily_SIC_august = SIC_daily(np.zeros((dimX,dimY,2,31)),np.zeros((dimX,dimY,2)))\n",
    "SIC_june = SIC_monthly(np.zeros((dimX,dimY,2016-1979+1)),np.zeros((dimXR,dimYR,2018-1979+1)),{},{},{},{},{},{})\n",
    "SIC_july = SIC_monthly(np.zeros((dimX,dimY,2016-1979+1)),np.zeros((dimXR,dimYR,2018-1979+1)),{},{},{},{},{},{})\n",
    "SIC_august = SIC_monthly(np.zeros((dimX,dimY,2016-1979+1)),np.zeros((dimXR,dimYR,2018-1979+1)),{},{},{},{},{},{})\n",
    "\n",
    "SIC_daily.read_daily(daily_SIC_june,\"06\",30, 2018)\n",
    "SIC_daily.read_daily(daily_SIC_july,\"07\",31, 2018)\n",
    "SIC_daily.read_daily(daily_SIC_august,\"08\",31, 2018)\n",
    "SIC_daily.create_monthly(daily_SIC_june, 2)\n",
    "SIC_daily.create_monthly(daily_SIC_july, 2)\n",
    "SIC_daily.create_monthly(daily_SIC_august, 2)\n",
    "SIC_monthly.read_monthly(SIC_june,\"06\")\n",
    "SIC_monthly.read_monthly(SIC_july,\"07\")\n",
    "SIC_monthly.read_monthly(SIC_august,\"08\")\n",
    "SIC_june.data = np.concatenate((SIC_june.data,daily_SIC_june.monthly),2)\n",
    "SIC_july.data = np.concatenate((SIC_july.data,daily_SIC_july.monthly),2)\n",
    "SIC_august.data = np.concatenate((SIC_august.data,daily_SIC_august.monthly),2)\n",
    "print('Re-gridding...')\n",
    "lon_regrid, lat_regrid = SIC_monthly.regrid(SIC_june, 6, lat, lon)\n",
    "lon_regrid, lat_regrid = SIC_monthly.regrid(SIC_july, 7, lat, lon)\n",
    "lon_regrid, lat_regrid = SIC_monthly.regrid(SIC_august, 8, lat, lon)\n",
    "print('De-trending...')\n",
    "SIC_monthly.detrend(SIC_june, 2018, SIC_june.regrid)\n",
    "SIC_monthly.detrend(SIC_july, 2018, SIC_july.regrid)\n",
    "SIC_monthly.detrend(SIC_august, 2018, SIC_august.regrid)\n",
    "SIC_monthly.gen_networks(SIC_june, \"june\", lat_regrid)\n",
    "SIC_monthly.gen_networks(SIC_july, \"july\", lat_regrid)\n",
    "SIC_monthly.gen_networks(SIC_august, \"august\", lat_regrid)\n",
    "\n",
    "june = GPR(np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)))\n",
    "july = GPR(np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)))\n",
    "august = GPR(np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)),np.zeros((len(regions),2018-1985+1)))\n",
    "\n",
    "GPR.forecast(june, SIEs_dt, SIEs_trend, \"june\", SIC_june.anomalies, 10)\n",
    "GPR.forecast(july, SIEs_dt, SIEs_trend, \"july\", SIC_july.anomalies, 10)\n",
    "GPR.forecast(august, SIEs_dt, SIEs_trend, \"august\", SIC_august.anomalies, 10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
